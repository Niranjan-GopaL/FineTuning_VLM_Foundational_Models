{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:42.097623Z",
     "iopub.status.busy": "2025-05-15T04:45:42.097023Z",
     "iopub.status.idle": "2025-05-15T04:45:42.107512Z",
     "shell.execute_reply": "2025-05-15T04:45:42.106767Z",
     "shell.execute_reply.started": "2025-05-15T04:45:42.097599Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/abo-vqa/abo-images-small.tar.xz\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/adapter_model.safetensors\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/preprocessor_config.json\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/adapter_config.json\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/README.md\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/tokenizer.json\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/tokenizer_config.json\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/special_tokens_map.json\n",
      "/kaggle/input/fine-tuned/fine_tune_blip_lora/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T11:48:32.863785Z",
     "iopub.status.busy": "2025-05-17T11:48:32.863160Z",
     "iopub.status.idle": "2025-05-17T11:48:59.984294Z",
     "shell.execute_reply": "2025-05-17T11:48:59.983169Z",
     "shell.execute_reply.started": "2025-05-17T11:48:32.863751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Collecting git+https://github.com/salesforce/LAVIS.git\n",
      "  Cloning https://github.com/salesforce/LAVIS.git to /tmp/pip-req-build-l6i04k_7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salesforce/LAVIS.git /tmp/pip-req-build-l6i04k_7\n",
      "  Resolved https://github.com/salesforce/LAVIS.git to commit 506965b9c4a18c1e565bd32acaccabe0198433f7\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting contexttimer (from salesforce-lavis==1.0.1)\n",
      "  Using cached contexttimer-0.3.3.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting decord (from salesforce-lavis==1.0.1)\n",
      "  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Collecting diffusers<=0.16.0 (from salesforce-lavis==1.0.1)\n",
      "  Using cached diffusers-0.16.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: einops>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.8.1)\n",
      "Collecting fairscale==0.4.4 (from salesforce-lavis==1.0.1)\n",
      "  Using cached fairscale-0.4.4.tar.gz (235 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ftfy (from salesforce-lavis==1.0.1)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting iopath (from salesforce-lavis==1.0.1)\n",
      "  Using cached iopath-0.1.10.tar.gz (42 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (7.34.0)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.3.0)\n",
      "Collecting opencv-python-headless==4.5.5.64 (from salesforce-lavis==1.0.1)\n",
      "  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting opendatasets (from salesforce-lavis==1.0.1)\n",
      "  Using cached opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (25.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.2.3)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (5.24.1)\n",
      "Collecting pre-commit (from salesforce-lavis==1.0.1)\n",
      "  Using cached pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pycocoevalcap (from salesforce-lavis==1.0.1)\n",
      "  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.0.8)\n",
      "Collecting python-magic (from salesforce-lavis==1.0.1)\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.25.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.2.0)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.8.5)\n",
      "Collecting streamlit (from salesforce-lavis==1.0.1)\n",
      "  Using cached streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting timm==0.4.12 (from salesforce-lavis==1.0.1)\n",
      "  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.21.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (4.67.1)\n",
      "Collecting transformers==4.33.2 (from salesforce-lavis==1.0.1)\n",
      "  Using cached transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "Collecting webdataset (from salesforce-lavis==1.0.1)\n",
      "  Using cached webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.45.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (2.6.0+cu124)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.13.1)\n",
      "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (1.0.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (3.9.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from salesforce-lavis==1.0.1) (0.14.0)\n",
      "Collecting easydict==1.9 (from salesforce-lavis==1.0.1)\n",
      "  Using cached easydict-1.9.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pyyaml_env_tag==0.1 (from salesforce-lavis==1.0.1)\n",
      "  Using cached pyyaml_env_tag-0.1-py3-none-any.whl.metadata (4.1 kB)\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes && pip install git+https://github.com/salesforce/LAVIS.git && pip install git+https://github.com/OFA-Sys/OFA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:42.114822Z",
     "iopub.status.busy": "2025-05-15T04:45:42.114590Z",
     "iopub.status.idle": "2025-05-15T04:45:45.449673Z",
     "shell.execute_reply": "2025-05-15T04:45:45.448817Z",
     "shell.execute_reply.started": "2025-05-15T04:45:42.114806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:45.452000Z",
     "iopub.status.busy": "2025-05-15T04:45:45.451704Z",
     "iopub.status.idle": "2025-05-15T04:45:45.456808Z",
     "shell.execute_reply": "2025-05-15T04:45:45.456079Z",
     "shell.execute_reply.started": "2025-05-15T04:45:45.451967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import Blip2Processor, BlipForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:45.457916Z",
     "iopub.status.busy": "2025-05-15T04:45:45.457718Z",
     "iopub.status.idle": "2025-05-15T04:45:45.649256Z",
     "shell.execute_reply": "2025-05-15T04:45:45.648455Z",
     "shell.execute_reply.started": "2025-05-15T04:45:45.457901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_abo_images  fine_tune_blip_lora  state.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! ls /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:45.650611Z",
     "iopub.status.busy": "2025-05-15T04:45:45.650345Z",
     "iopub.status.idle": "2025-05-15T04:45:45.822340Z",
     "shell.execute_reply": "2025-05-15T04:45:45.821619Z",
     "shell.execute_reply.started": "2025-05-15T04:45:45.650586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter_config.json\t   README.md\t\t    tokenizer_config.json\n",
      "adapter_model.safetensors  special_tokens_map.json  tokenizer.json\n",
      "preprocessor_config.json   THIS_IS_JUST_A_FLAG\t    vocab.txt\n"
     ]
    }
   ],
   "source": [
    "! ls /kaggle/working/fine_tune_blip_lora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:45.825122Z",
     "iopub.status.busy": "2025-05-15T04:45:45.824860Z",
     "iopub.status.idle": "2025-05-15T04:45:45.996149Z",
     "shell.execute_reply": "2025-05-15T04:45:45.995456Z",
     "shell.execute_reply.started": "2025-05-15T04:45:45.825097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜/kaggle/working/fine_tune_blip_lora/THIS_IS_JUST_A_FLAGâ€™: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir /kaggle/working/fine_tune_blip_lora/THIS_IS_JUST_A_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:45.997381Z",
     "iopub.status.busy": "2025-05-15T04:45:45.997112Z",
     "iopub.status.idle": "2025-05-15T04:45:46.196099Z",
     "shell.execute_reply": "2025-05-15T04:45:46.195416Z",
     "shell.execute_reply.started": "2025-05-15T04:45:45.997342Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp -r \"/kaggle/input/fine-tuned/fine_tune_blip_lora\"  \"/kaggle/working/.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:46.197379Z",
     "iopub.status.busy": "2025-05-15T04:45:46.197155Z",
     "iopub.status.idle": "2025-05-15T04:45:46.202396Z",
     "shell.execute_reply": "2025-05-15T04:45:46.201603Z",
     "shell.execute_reply.started": "2025-05-15T04:45:46.197356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---- Configuration ----\n",
    "CSV_FOLDER = \"/kaggle/working/extracted_abo_images/abo-images-small/images/dataset_csv\"\n",
    "IMAGE_DIR = \"/kaggle/working/extracted_abo_images/abo-images-small/images/small\"\n",
    "MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "OUTPUT_DIR = \"/kaggle/working/fine_tune_blip_lora\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T04:45:46.203375Z",
     "iopub.status.busy": "2025-05-15T04:45:46.203174Z",
     "iopub.status.idle": "2025-05-15T04:45:46.397617Z",
     "shell.execute_reply": "2025-05-15T04:45:46.396977Z",
     "shell.execute_reply.started": "2025-05-15T04:45:46.203360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80336 rows from CSVs\n",
      "New 79990 rows from CSVs\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    pd.read_csv(os.path.join(CSV_FOLDER, f))\n",
    "    for f in os.listdir(CSV_FOLDER) if f.endswith(\".csv\")\n",
    "])\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from CSVs\")\n",
    "df = df[df['answer'].notnull()]\n",
    "df = df[df['image_id'].notnull()]\n",
    "df = df[df['path'].notnull()]\n",
    "df = df[df['question'].notnull()]\n",
    "print(f\"New {len(df)} rows from CSVs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T11:50:32.474539Z",
     "iopub.status.busy": "2025-05-17T11:50:32.474228Z",
     "iopub.status.idle": "2025-05-17T11:53:45.165010Z",
     "shell.execute_reply": "2025-05-17T11:53:45.164201Z",
     "shell.execute_reply.started": "2025-05-17T11:50:32.474516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Initial rows: 80336\n",
      "Clean rows: 79990\n",
      "Working with sample of 100 rows\n",
      "\n",
      "=== Evaluating BLIP ===\n",
      "\n",
      "Loading BLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 11:50:42.384675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747482642.407432     243 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747482642.414680     243 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Running BLIP: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:10<00:00,  9.79it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d01b9f3429c4e9bb8890b91981a076d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaf646944da44bca9bb11a2c9e3901e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dc7fb411344432bb36707ee51f94c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0f0ce10ced492b805bc5abf3166212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe19dc9d3ed4b16900c2978bfe67153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb3e81768ab48409dc4a99f4ce99b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” BLIP BERTScore:\n",
      " - Precision: 0.9347\n",
      " - Recall:    0.9261\n",
      " - F1 Score:  0.9292\n",
      "\n",
      "=== Evaluating BLIP2 ===\n",
      "\n",
      "Loading BLIP-2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a213e3d74ea4a5691cfc304399f48e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076b6c497eac49caafbbc425972888a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/882 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0a59e5c46b4c03adf0b9302181d7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1d4981ada44a36bb3c1ea4e844db4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f1915bef9e411a92c529bd21a724fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615746087c5648e2a2d326063fe1dc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dd18d241ff4e769731d1a462b799ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8a668d6bc147a6b6cc41db67ca6c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c5c1f60a0642129dcecbb6b8079b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912ed343a52a4194a9b87ea9a897ad07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/122k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f964b6a1c9e74d0ea42a4a4d5c03ffc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8fb695e4274ced8ad9857c50655192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab007a1c42243908354e388559cdbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34eafd7f38e646ef9791c39388590209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e09ebbae9b24ea993ab4387e9664897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running BLIP2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:11<00:00,  8.79it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” BLIP2 BERTScore:\n",
      " - Precision: 0.8042\n",
      " - Recall:    0.8183\n",
      " - F1 Score:  0.8108\n",
      "\n",
      "=== Evaluating OFA ===\n",
      "\n",
      "Loading OFA model...\n",
      "Failed to evaluate OFA: cannot import name 'OFATokenizer' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)\n",
      "\n",
      "=== Evaluating ViLBERT ===\n",
      "\n",
      "Loading ViLBERT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70aef00d52144d0b89ff6a2e3b0e4875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb101028c6474e329c9471ac1121ed53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29b25db6fa44c94a0124e6d5e08de4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50920af7cdb94def9d21395c37255120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d69639ac2074cb49727f6ac9c71f7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04713af43ee24513bad0b0562e30fa22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2a73409956427ea546d7c16d0184a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ViLBERT:  11%|â–ˆ         | 11/100 [00:00<00:04, 21.25it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cd0512a2c24122ae013c70212f3979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ViLBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 25.94it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” ViLBERT BERTScore:\n",
      " - Precision: 0.9467\n",
      " - Recall:    0.9284\n",
      " - F1 Score:  0.9363\n",
      "\n",
      "=== Evaluating CLIP ===\n",
      "\n",
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f193a1c5f9f4712839914dea92cee2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ccee488ad7414e91257f8484e2a853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518a7c8820db43338c22b97ff1b4ff6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564de40e514342f0924c60075553c260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb062601fe984e2d9d4104298cf7378e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c3ceebce3b4748b97ce0d3cb156730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9682b5f7a89423c843e07bafcd76882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc63bd31a5d4a50adea8823b151bfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2547964f6a4351a9ed289184e8f6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running CLIP:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Running CLIP:   1%|          | 1/100 [00:00<00:13,  7.19it/s]\u001b[A\n",
      "Running CLIP:   5%|â–Œ         | 5/100 [00:00<00:04, 22.08it/s]\u001b[A\n",
      "Running CLIP:   8%|â–Š         | 8/100 [00:00<00:04, 21.27it/s]\u001b[A\n",
      "Running CLIP:  11%|â–ˆ         | 11/100 [00:00<00:03, 23.21it/s]\u001b[A\n",
      "Running CLIP:  14%|â–ˆâ–        | 14/100 [00:00<00:03, 23.30it/s]\u001b[A\n",
      "Running CLIP:  18%|â–ˆâ–Š        | 18/100 [00:00<00:03, 25.84it/s]\u001b[A\n",
      "Running CLIP:  22%|â–ˆâ–ˆâ–       | 22/100 [00:00<00:02, 27.32it/s]\u001b[A\n",
      "Running CLIP:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:01<00:02, 25.65it/s]\u001b[A\n",
      "Running CLIP:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:01<00:02, 28.53it/s]\u001b[A\n",
      "Running CLIP:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:01<00:02, 30.51it/s]\u001b[A\n",
      "Running CLIP:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:01<00:02, 31.22it/s]\u001b[A\n",
      "Running CLIP:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:01<00:02, 28.80it/s]\u001b[A\n",
      "Running CLIP:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:01<00:01, 29.84it/s]\u001b[A\n",
      "Running CLIP:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:01<00:01, 27.27it/s]\u001b[A\n",
      "Running CLIP:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:01<00:01, 25.65it/s]\u001b[A\n",
      "Running CLIP:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:02<00:01, 24.30it/s]\u001b[A\n",
      "Running CLIP:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:02<00:01, 24.84it/s]\u001b[A\n",
      "Running CLIP:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:02<00:01, 25.71it/s]\u001b[A\n",
      "Running CLIP:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:02<00:01, 27.56it/s]\u001b[A\n",
      "Running CLIP:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:02<00:01, 29.92it/s]\u001b[A\n",
      "Running CLIP:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:02<00:00, 34.59it/s]\u001b[A\n",
      "Running CLIP:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:02<00:00, 38.30it/s]\u001b[A\n",
      "Running CLIP:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:02<00:00, 42.17it/s]\u001b[A\n",
      "Running CLIP:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:03<00:00, 44.98it/s]\u001b[A\n",
      "Running CLIP: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 31.20it/s][A\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” CLIP BERTScore:\n",
      " - Precision: 0.7797\n",
      " - Recall:    0.8433\n",
      " - F1 Score:  0.8098\n",
      "\n",
      "Results saved to vqa_model_comparison_results.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Initial Setup ===\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from bert_score import score as bert_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "CSV_FOLDER = \"/kaggle/working/extracted_abo_images/abo-images-small/images/dataset_csv\"\n",
    "IMAGE_DIR = \"/kaggle/working/extracted_abo_images/abo-images-small/images/small\"\n",
    "BATCH_SIZE = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.concat([\n",
    "    pd.read_csv(os.path.join(CSV_FOLDER, f))\n",
    "    for f in os.listdir(CSV_FOLDER) if f.endswith(\".csv\")\n",
    "])\n",
    "print(f\"Initial rows: {len(df)}\")\n",
    "\n",
    "# Clean data\n",
    "df = df[df['answer'].notnull() & df['image_id'].notnull() & df['path'].notnull() & df['question'].notnull()]\n",
    "print(f\"Clean rows: {len(df)}\")\n",
    "\n",
    "# For testing, let's take a small sample (remove this for full run)\n",
    "df = df.sample(100, random_state=42).reset_index(drop=True)\n",
    "print(f\"Working with sample of {len(df)} rows\")\n",
    "\n",
    "# === Model Loading Functions ===\n",
    "def load_blip_model():\n",
    "    print(\"\\nLoading BLIP model...\")\n",
    "    from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "    model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "    return processor, model\n",
    "\n",
    "def load_blip2_model():\n",
    "    print(\"\\nLoading BLIP-2 model...\")\n",
    "    from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(device)\n",
    "    return processor, model\n",
    "\n",
    "def load_ofa_model():\n",
    "    print(\"\\nLoading OFA model...\")\n",
    "    from transformers import OFATokenizer, OFAModel\n",
    "    from transformers.models.ofa.generate import sequence_generator\n",
    "    tokenizer = OFATokenizer.from_pretrained(\"OFA-Sys/OFA-medium\")\n",
    "    model = OFAModel.from_pretrained(\"OFA-Sys/OFA-medium\", use_cache=True).to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_vilbert_model():\n",
    "    print(\"\\nLoading ViLBERT model...\")\n",
    "    from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "    processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "    model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").to(device)\n",
    "    return processor, model\n",
    "\n",
    "def load_clip_model():\n",
    "    print(\"\\nLoading CLIP model...\")\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    return processor, model\n",
    "\n",
    "# === Model Inference Functions ===\n",
    "def run_blip_inference(processor, model, image, question):\n",
    "    inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def run_blip2_inference(processor, model, image, question):\n",
    "    inputs = processor(image, question, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def run_ofa_inference(tokenizer, model, image, question):\n",
    "    inputs = tokenizer([question], return_tensors=\"pt\").input_ids\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    patch_img = tokenizer(img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs.to(device), patch_images=patch_img[\"pixel_values\"].to(device))\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "def run_vilbert_inference(processor, model, image, question):\n",
    "    encoding = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    idx = logits.argmax(-1).item()\n",
    "    return model.config.id2label[idx]\n",
    "\n",
    "def run_clip_inference(processor, model, image, question):\n",
    "    # CLIP isn't a VQA model, but we can use it for zero-shot classification\n",
    "    # This is a simplified approach - not ideal for VQA\n",
    "    inputs = processor(text=[question], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    return \"CLIP-not-proper-VQA\"  # Simplified for demo\n",
    "\n",
    "# === Main Evaluation Loop ===\n",
    "def evaluate_models(df, models_to_run):\n",
    "    results = df.copy()\n",
    "    \n",
    "    for model_name, (load_func, run_func) in models_to_run.items():\n",
    "        print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "        try:\n",
    "            # Load model\n",
    "            processor, model = load_func()\n",
    "            \n",
    "            # Run inference\n",
    "            predictions = []\n",
    "            for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Running {model_name}\"):\n",
    "                image_path = os.path.join(IMAGE_DIR, row[\"path\"])\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    pred = run_func(processor, model, image, row[\"question\"])\n",
    "                    predictions.append(pred)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {str(e)}\")\n",
    "                    predictions.append(\"ERROR\")\n",
    "            \n",
    "            # Add predictions to results\n",
    "            results[f\"{model_name}_predicted\"] = predictions\n",
    "            \n",
    "            # Compute BERTScore\n",
    "            P, R, F1 = bert_score(\n",
    "                cands=predictions,\n",
    "                refs=results['answer'].tolist(),\n",
    "                lang=\"en\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{model_name} BERTScore:\")\n",
    "            print(f\" - Precision: {P.mean():.4f}\")\n",
    "            print(f\" - Recall:    {R.mean():.4f}\")\n",
    "            print(f\" - F1 Score:  {F1.mean():.4f}\")\n",
    "            \n",
    "            # Free up memory\n",
    "            del processor, model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to evaluate {model_name}: {str(e)}\")\n",
    "            results[f\"{model_name}_predicted\"] = [\"ERROR\"] * len(df)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# === Define Models to Evaluate ===\n",
    "models_to_run = {\n",
    "    \"BLIP\": (load_blip_model, run_blip_inference),\n",
    "    \"BLIP2\": (load_blip2_model, run_blip2_inference),\n",
    "    \"OFA\": (load_ofa_model, run_ofa_inference),\n",
    "    \"ViLBERT\": (load_vilbert_model, run_vilbert_inference),\n",
    "    \"CLIP\": (load_clip_model, run_clip_inference)\n",
    "}\n",
    "\n",
    "# === Run Evaluation ===\n",
    "results_df = evaluate_models(df, models_to_run)\n",
    "\n",
    "# === Save Results ===\n",
    "output_path = \"vqa_model_comparison_results.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to {output_path}\")\n",
    "\n",
    "# === Summary Report ===\n",
    "print(\"\\n=== Final Summary ===\")\n",
    "for model_name in models_to_run.keys():\n",
    "    col_name = f\"{model_name}_predicted\"\n",
    "    if col_name in results_df.columns:\n",
    "        correct = (results_df[col_name].str.lower() == results_df['answer'].str.lower()).sum()\n",
    "        accuracy = correct / len(results_df)\n",
    "        print(f\"{model_name} Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7404097,
     "sourceId": 11791686,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7418762,
     "sourceId": 11811969,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
